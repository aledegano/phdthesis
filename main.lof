\select@language {italian}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Number of transistor count in logarithmic scale with respect to the year of introduction. Credit to \href {https://commons.wikimedia.org/wiki/User:Wgsimon}{Wgsimon}, \href {http://creativecommons.org/licenses/by-sa/3.0/}{CC BY-SA 3.0}}}{3}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Maximum theoretical speedup for various values of $f$ as a function of the number of parallel processes. Credit to \href {https://en.wikipedia.org/wiki/User:Daniels220}{Daniels220}, \href {http://creativecommons.org/licenses/by-sa/3.0/}{CC BY-SA 3.0}}}{5}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces a) Scheme of a CPU where most of the transistors are dedicated to caches and control unit. b) Scheme of a GPU with most of the resources used for the numerous execution units.}}{5}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces A NVIDIA Kepler GPU equipped with 192 single-precision CUDA cores.}}{7}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of two CUDA streams execution.}}{10}{figure.2.5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Example of a 2D tree building. $1^{st}$ step: find the middle point with respect to the $x$ axis and split the area in two equal sub-areas with a line passing through the point (blue line). $2^{nd}$ step: in each children find the median points and split further the children by a line constant in $y$ passing through them (red lines). $N^{th}$ step: all points belong to a leaf and all volumes are created.}}{12}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Volume Kd-tree build timings.}}{19}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Volume Kd-tree search times for CPU sequential and GPU parallel code}}{20}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Volume Kd-tree speedup between CPU sequential and GPU parallel search code}}{21}{figure.3.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Interleaving of four asyncronous CUDA streams. H2D: host to device process, D2H: device to host, Kernel: code execution.}}{28}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Time spent by the build of the KD-tree.}}{29}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Relative performance gain between multiple CUDA streams with respect to the single stream.}}{30}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Search times for CPU sequential and GPU parallel code.}}{30}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Speedup between CPU sequential and GPU parallel code.}}{31}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Search times for CPU sequential and GPU parallel code for fixed maximum number of nearest neighbors.}}{32}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Speedup between CPU sequential and GPU parallel code for fixed maximum number of nearest neighbors.}}{32}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Time spent by the build of the KD-tree on simulated data.}}{33}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Search times for CPU sequential and GPU parallel code on simulated data.}}{34}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Speedup between CPU sequential and GPU parallel code on simulated data.}}{34}{figure.4.10}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Kernel execution time for different number of CUDA streams.}}{37}{figure.4.11}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Setup of the Tegra K1 board and the power meter.}}{41}{figure.5.1}
\addvspace {10\p@ }
