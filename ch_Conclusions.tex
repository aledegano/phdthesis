\chapter{Conclusions}\label{ch:conclusion}
The HL-LHC conditions of Phase-II will produce a great quantity of interactions in each event, rising the average pileup to 140 with spikes of 200. To ready the detector for Phase-II the CMS electromagnetic calorimeters endcaps will be upgraded with a new highly segmented sampling calorimeter: HGCAL. As we showed in Chapter \ref{ch:intro} for the above reasons it is expected that both the online and offline reconstructions will have to process a big number of \textit{rechits} in each event. Indeed, to reconstruct the electromagnetic clusters, it will be necessary to search for all the nearest neighbors of between 150000 and 350000 points, and for that reason the computational power required will need to increase substantially with respect to what it is available today.
\vspace{0.5cm}
\\
In Chapter \ref{ch:gpu} we described the most relevant differences between a Central Processing Unit and a Graphical Processing Unit. We showed how GPUs, that where made for a completely different purpose, can be leveraged to execute massively parallel programs thanks to the high number of cores they features. We described how a particular brand, NVIDIA, provides an extensive programming API for their GPU called CUDA and which features of CUDA can really benefit the parallel execution of a program.
\vspace{0.5cm}
\\
One of the most promising algorithms to perform the all nearest neighbors search is the binary K-Dimensional tree, in short KD-tree. In Chapter \ref{ch:volume_kd} we described the general properties of a KD-tree, how it is built and how it can be traversed to perform the nearest neighbors search. We also showed an implementation, which we refers to as Volume KD-tree since it uses three-dimensional volume as nodes of the tree, featuring a sequential search that runs on CPU and a parallel one for the GPU. After a thorough performance analysis we observed that the shortcomings of this implementation where outweighing the features meant to increase the performances and moved on to a different implementation.
\vspace{0.5cm}
\\
Abandoning the idea of tree nodes as volumes and converting the recursions into iterations we designed an entirely different KD-tree in Chapter \ref{ch:fkdtree}. The implementation proved to be much more suited to high parallelism and even more extensive performance testing where done. Beginning with random distributed points the algorithm showed very promising results in terms of both the absolute timing and the parallel code with respect to the sequential one. Another test using simulated data from the CMS software was executed and the good performance was still observed: the GPU code of the KD-tree is able to search the nearest neighbors of all 345000 simulated rechits in 37 $\unit{ms}$, with an effective speedup of almost 14 with respect to the sequential CPU code.
\vspace{0.5cm}
\\
We evaluated the energy consumption of the latter KD-tree on the workstation used throughout most of the work and also on the NVIDIA TK1 development board in Chapter \ref{ch:power}. First we compared the energy used by the workstation CPU (an Intel i7-3770) with the energy used by the GPU board mounted on the same machine (a NVIDIA Tesla k40c). We observed that, despite the power employment of the CPU is scarce, the overall energy consumed is significantly lower for the GPU with $13 \pm 1 \unit{J/search}$ respect to the $88 \pm 1 \unit{J/search}$ required by the CPU. Similar results where found when comparing the TK1 CPU (an ARM Cortex A15) and its integrated GPU (NVIDIA Kepler). While the ARM CPU results where really unimpressive, requiring $219 \pm 3 \unit{J/search}$, the GPU once again proved to be very efficient with $11 \pm 1 \unit{J/search}$. Therefore in the chapter we demonstrated how a well designed algorithm running on GPU not only can have great performance but can also significantly limit the energy required to perform a full nearest neighbors search. Of particular interest are the very compact System-on-a-Chip like the one featured in the TK1: with a very small form factor and a very low power usage those systems could be mounted in great quantities in small spaces without requiring complex heat dissipation systems, thus making them a very interesting candidate for a dedicated High Level Trigger farm for HGCAL.
\vspace{0.5cm}
\\
Although in the years to come before the Long Shutdown 3 the computational hardware will certainly improve and possibly in ways that are not entirely predictable, we strongly believe that the parallel programming paradigm will remains a stronghold of the High Performance Computing. We clearly showed throughout this work how an efficient parallel program might be not trivial to design. As a direct consequence of the Moore's law still proving to be true, the future will bring both CPUs and GPUs with even more cores and soon designing the software to be massively parallel will become essential.
\vspace{0.5cm}
\\
The full code developed for this work can be freely consulted and downloaded online \cite{github}.